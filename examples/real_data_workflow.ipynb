{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22e63007",
   "metadata": {},
   "source": [
    "# Assessing Policies Using Real Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9ada81",
   "metadata": {},
   "source": [
    "In this workflow, CFRL takes in an offline trajectory and then preprocesses the offline trajectory using `SyntheticPreprocessor`. After that, the preprocessed trajectory is passed into `FQI` to train a counterfactually fair policy, which is then assessed using :code:`evaluate_reward_through_fqe()` and `evaluate_fairness_through_model()` based on a `SimulatedEnvironment` that mimics the transition rules of the true environment underlying the training trajectory. The final output of the workflow is the policy trained on the preprocessed data as well as its estimated value and counterfactual fairness metric. This workflow is appropriate when the user is interested in knowing the value and counterfactual fairness achieved by the trained policy when interacting with the true underlying environment.\n",
    "\n",
    "We begin by importing the liberaries needed for this demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78109a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need this temporarily to import CFRL before it is officially published to PyPI\n",
    "import sys\n",
    "sys.path.append(\"E:/learning/university/MiSIL/CFRL Python Package/CFRL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e0d56e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x23d0a6ca650>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from CFRL.reader import read_trajectory_from_dataframe\n",
    "from CFRL.preprocessor import SequentialPreprocessor\n",
    "from CFRL.agents import FQI\n",
    "from CFRL.environment import SimulatedEnvironment\n",
    "from CFRL.evaluation import evaluate_reward_through_fqe, evaluate_fairness_through_model\n",
    "np.random.seed(1) # ensure reproducibility\n",
    "torch.manual_seed(1) # ensure reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09c8df4",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d136b2d7",
   "metadata": {},
   "source": [
    "In this demonstration, we use an offline trajectory generated from a `SyntheticEnvironment` using some pre-specified transition rules. Although it is actually synthesized, we treat it as if it is from some unknown environment for pedagogical convenience in this demonstration.\n",
    "\n",
    "The trajectory contains 500 individuals (i.e. $N=500$) and 10 transitions (i.e. $T=10$). The actions are binary (0 or 1) and were sampled using a random policy that selects 0 or 1 randomly with equal probability. It is stored in a tabular format in a `.csv` file. The sensitive attribute variable is bivariate, stored in columns `z1` and `z2`. The legit values of the sensitive attribute are $[0, 0]$, $[1, 0]$, $[0, 1]$, and $[1, 1]$. The state variable is tri-variate, stored in columns `state1`, `state2`, and `state3`. The actions are stored in the column `action` and rewards in the column `reward`. The tabular data also includes an extra irrelevant column `timestamp`. \n",
    "\n",
    "We can load and view the tabular data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8affb9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ID</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>z1</th>\n",
       "      <th>z2</th>\n",
       "      <th>action</th>\n",
       "      <th>reward</th>\n",
       "      <th>state1</th>\n",
       "      <th>state2</th>\n",
       "      <th>state3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.124345</td>\n",
       "      <td>-0.111756</td>\n",
       "      <td>-0.028172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.380339</td>\n",
       "      <td>-0.071876</td>\n",
       "      <td>0.545250</td>\n",
       "      <td>-0.020279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.849111</td>\n",
       "      <td>-1.084077</td>\n",
       "      <td>-1.696634</td>\n",
       "      <td>-1.179136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.421291</td>\n",
       "      <td>-2.317520</td>\n",
       "      <td>-1.787875</td>\n",
       "      <td>-2.148363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-5.142691</td>\n",
       "      <td>-2.936506</td>\n",
       "      <td>-3.603797</td>\n",
       "      <td>-3.590126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5495</th>\n",
       "      <td>5495</td>\n",
       "      <td>500.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-12.563265</td>\n",
       "      <td>-4.024293</td>\n",
       "      <td>-6.587401</td>\n",
       "      <td>-3.859436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5496</th>\n",
       "      <td>5496</td>\n",
       "      <td>500.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-14.073520</td>\n",
       "      <td>-5.952644</td>\n",
       "      <td>-5.854450</td>\n",
       "      <td>-4.218220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5497</th>\n",
       "      <td>5497</td>\n",
       "      <td>500.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-16.691358</td>\n",
       "      <td>-5.687570</td>\n",
       "      <td>-6.008377</td>\n",
       "      <td>-5.618730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5498</th>\n",
       "      <td>5498</td>\n",
       "      <td>500.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-18.394408</td>\n",
       "      <td>-7.551435</td>\n",
       "      <td>-6.816310</td>\n",
       "      <td>-6.740886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5499</th>\n",
       "      <td>5499</td>\n",
       "      <td>500.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-20.989511</td>\n",
       "      <td>-6.202696</td>\n",
       "      <td>-8.487149</td>\n",
       "      <td>-7.020361</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5500 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0     ID  timestamp   z1   z2  action     reward    state1  \\\n",
       "0              0    1.0        1.0  0.0  0.0     NaN        NaN  2.124345   \n",
       "1              1    1.0        2.0  0.0  0.0     1.0   3.380339 -0.071876   \n",
       "2              2    1.0        3.0  0.0  0.0     0.0   1.849111 -1.084077   \n",
       "3              3    1.0        4.0  0.0  0.0     0.0  -4.421291 -2.317520   \n",
       "4              4    1.0        5.0  0.0  0.0     1.0  -5.142691 -2.936506   \n",
       "...          ...    ...        ...  ...  ...     ...        ...       ...   \n",
       "5495        5495  500.0        7.0  0.0  0.0     0.0 -12.563265 -4.024293   \n",
       "5496        5496  500.0        8.0  0.0  0.0     0.0 -14.073520 -5.952644   \n",
       "5497        5497  500.0        9.0  0.0  0.0     0.0 -16.691358 -5.687570   \n",
       "5498        5498  500.0       10.0  0.0  0.0     0.0 -18.394408 -7.551435   \n",
       "5499        5499  500.0       11.0  0.0  0.0     1.0 -20.989511 -6.202696   \n",
       "\n",
       "        state2    state3  \n",
       "0    -0.111756 -0.028172  \n",
       "1     0.545250 -0.020279  \n",
       "2    -1.696634 -1.179136  \n",
       "3    -1.787875 -2.148363  \n",
       "4    -3.603797 -3.590126  \n",
       "...        ...       ...  \n",
       "5495 -6.587401 -3.859436  \n",
       "5496 -5.854450 -4.218220  \n",
       "5497 -6.008377 -5.618730  \n",
       "5498 -6.816310 -6.740886  \n",
       "5499 -8.487149 -7.020361  \n",
       "\n",
       "[5500 rows x 10 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trajectory = pd.read_csv('../data/sample_data_large_multi.csv')\n",
    "trajectory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713fd846",
   "metadata": {},
   "source": [
    "We now read the trajectory from the tabular format into Trajectory Arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2600437b",
   "metadata": {},
   "outputs": [],
   "source": [
    "zs, states, actions, rewards, ids = read_trajectory_from_dataframe(\n",
    "                                                data=trajectory, \n",
    "                                                z_labels=['z1', 'z2'], \n",
    "                                                state_labels=['state1', 'state2', 'state3'], \n",
    "                                                action_label='action', \n",
    "                                                reward_label='reward', \n",
    "                                                id_label='ID', \n",
    "                                                T=10\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aae8c78a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"zs, states, actions, rewards, ids = read_trajectory_from_dataframe(\\n                                                data=trajectory, \\n                                                z_labels=['z1'], \\n                                                state_labels=['state1'], \\n                                                action_label='action', \\n                                                reward_label='reward', \\n                                                id_label='ID', \\n                                                T=10\\n                                                )\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''zs, states, actions, rewards, ids = read_trajectory_from_dataframe(\n",
    "                                                data=trajectory, \n",
    "                                                z_labels=['z1'], \n",
    "                                                state_labels=['state1'], \n",
    "                                                action_label='action', \n",
    "                                                reward_label='reward', \n",
    "                                                id_label='ID', \n",
    "                                                T=10\n",
    "                                                )'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f429bf34",
   "metadata": {},
   "source": [
    "## Train-test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4550d0",
   "metadata": {},
   "source": [
    "We split the trajectory data into a training set (80%) and a testing set (20%). The training set is used to train the policy, while the testing set is used to evaluate the value and counterfactual fairness metric achieved by the policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1849086",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    zs_train, zs_test, \n",
    "    states_train, states_test, \n",
    "    actions_train, actions_test, \n",
    "    rewards_train, rewards_test\n",
    ") = train_test_split(zs, states, actions, rewards, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e569dad1",
   "metadata": {},
   "source": [
    "## Preprocessor Training & Trajectory Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9477df69",
   "metadata": {},
   "source": [
    "We now train the preprocessor and preprocess the trajectory. Note that if we train the preprocessor using only a subset of the data and preprocess the remaining subset of the data, then the resulting preprocessed trajectory might be too small to be useful for policy learning. We essentially want to preprocess as many individuals as possible. Fortunately, we can directly preprocess all individuals using the `train_preprocessor()` function when we set `cross_folds` to a relatively large number.\n",
    "\n",
    "When `cross_folds=K` where `K` is greater than 1, `train_preprocessor()` will internally divide the training data into `K` folds. For each $i=1,\\dots,k$, it trains a transition dynamics model based on all the folds other than the $i$-th one, and this model is then used to preprocess data in the $i$-th fold. This results in `K` folds of preprocessed data, each of which is processed using a model that is trained on the other folds. These `K` folds of preprocessed data are then combined and returned by `train_preprocessor()`. This method allows us to preprocess all individuals in the trajectory while reducing overfitting.\n",
    "\n",
    "To use this functionality, we first initialize a `SequentialPreprocessor` with `cross_folds` greater than 1. We use `cross_folds=5` here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9f2aa82",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = SequentialPreprocessor(z_space=[[0, 0], [0, 1], [1, 0], [1, 1]], \n",
    "                            num_actions=2, \n",
    "                            cross_folds=5, \n",
    "                            mode='single', \n",
    "                            reg_model='nn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ab09c42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"sp = SequentialPreprocessor(z_space=[[0], [1]], \\n                            num_actions=2, \\n                            cross_folds=5, \\n                            mode='single', \\n                            reg_model='nn')\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''sp = SequentialPreprocessor(z_space=[[0], [1]], \n",
    "                            num_actions=2, \n",
    "                            cross_folds=5, \n",
    "                            mode='single', \n",
    "                            reg_model='nn')'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e82e21",
   "metadata": {},
   "source": [
    "We now simultaneously train the preprocessor and preprocess all individuals in the trajectory using the precedure described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f097e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:47<00:00, 21.08it/s]\n",
      "100%|██████████| 1000/1000 [00:51<00:00, 19.55it/s]\n",
      "100%|██████████| 1000/1000 [01:00<00:00, 16.60it/s]\n",
      "100%|██████████| 1000/1000 [00:45<00:00, 22.22it/s]\n",
      "100%|██████████| 1000/1000 [00:42<00:00, 23.81it/s]\n"
     ]
    }
   ],
   "source": [
    "states_tilde, rewards_tilde = sp.train_preprocessor(zs=zs_train, \n",
    "                                                    xs=states_train, \n",
    "                                                    actions=actions_train, \n",
    "                                                    rewards=rewards_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1682729",
   "metadata": {},
   "source": [
    "## Policy Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326b046e",
   "metadata": {},
   "source": [
    "Now we train a policy using `FQI` and the preprocessed data. Note that although we passed `sp` into `agent` as an internal preprocessor, we set `preprocess=False` during training because the training data `state_tilde` and `rewards_tilde` are already preprocessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5b7cb50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:16<00:00,  1.30it/s]\n"
     ]
    }
   ],
   "source": [
    "agent = FQI(num_actions=2, model_type='nn', preprocessor=sp)\n",
    "agent.train(zs=zs_train, \n",
    "            xs=states_tilde, \n",
    "            actions=actions_train, \n",
    "            rewards=rewards_tilde, \n",
    "            max_iter=100, \n",
    "            preprocess=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b243926",
   "metadata": {},
   "source": [
    "## `SimulatedEnvironment` Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfdddb8",
   "metadata": {},
   "source": [
    "Before moving on to the evaluation stage, there is one more thing to do: We need to train a `SimulatedEnvironment` that mimics the transition rules of the true environment that generated the training trajectory, which will be used by the evaluation functions. To do so, we initialize a `SimulatedEnvironment` and train it on the whole trajectory data (i.e. training set and testing set combined)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "599323d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 44/1000 [00:00<00:17, 53.74it/s]\n",
      "  3%|▎         | 29/1000 [00:00<00:13, 69.58it/s]\n",
      "  2%|▏         | 22/1000 [00:00<00:17, 57.40it/s]\n",
      "  5%|▍         | 46/1000 [00:00<00:20, 46.88it/s]\n",
      "  6%|▌         | 57/1000 [00:01<00:17, 55.46it/s]\n",
      "  6%|▌         | 56/1000 [00:00<00:13, 70.80it/s]\n",
      "  5%|▌         | 52/1000 [00:00<00:16, 59.10it/s]\n",
      "  5%|▍         | 46/1000 [00:00<00:18, 52.30it/s]\n"
     ]
    }
   ],
   "source": [
    "env = SimulatedEnvironment(num_actions=2, \n",
    "                           state_model_type='nn', \n",
    "                           reward_model_type='nn')\n",
    "env.fit(zs=zs, states=states, actions=actions, rewards=rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d823b9cc",
   "metadata": {},
   "source": [
    "# Value Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b309b993",
   "metadata": {},
   "source": [
    "We now estimate the value achieved by the trained policy when interacting with the target environment using fitted Q evaluation (FQE), which is provided by `evaluate_value_through_fqe()`. We use the testing set for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0d38237",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [02:03<00:00,  1.62it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-48.450016"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value = evaluate_reward_through_fqe(zs=zs_test, \n",
    "                                    states=states_test, \n",
    "                                    actions=actions_test, \n",
    "                                    rewards=rewards_test, \n",
    "                                    policy=agent, \n",
    "                                    model_type='nn')\n",
    "value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff2a9ff",
   "metadata": {},
   "source": [
    "## Counterfactual Fairness Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57928dd",
   "metadata": {},
   "source": [
    "We now estimate the counterfactual fairness acheived by the policy when interacting with the target environment. To do so, we use `evaluate_fairness_through_model()`. This function first estimates the counterfactual trajectories of each individual in the data under a set of legit sensitive attribute values. Then it takes actions based on the counterfactual states using the policy that is to be evaluated. In the end, it calculates and returns a counterfactual fairness metric (CF metric) following the formula \n",
    "\n",
    "$\\max_{z', z \\in eval(Z)} \\frac{1}{NT} \\sum_{i=1}^{N} \\sum_{t=1}^{T} \\mathbb{I} \\left( A_t^{Z \\leftarrow z'}\\left(\\bar{U}_t(h_{it})\\right) \\neq A_t^{Z \\leftarrow z}\\left(\\bar{U}_t(h_{it})\\right) \\right),$\n",
    "\n",
    "where $eval(Z)$ is the set of sensitive attribute values passed in by `z_eval_levels`, $A_t^{Z \\leftarrow z'}\\left(\\bar{U}_t(h_{it})\\right)$ is the action taken in the counterfactual trajectory under $Z=z'$, and $A_t^{Z \\leftarrow z}\\left(\\bar{U}_t(h_{it})\\right)$ is the action taken under the counterfactual trajectory under $Z=z$. The CF metric is bounded between 0 and 1, with 0 representing perfect fairness and 1 indicating complete unfairness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98c68971",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0009090909090909091"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cf_metric = evaluate_fairness_through_model(env=env, \n",
    "                                            zs=zs_test, \n",
    "                                            states=states_test, \n",
    "                                            actions=actions_test, \n",
    "                                            policy=agent)\n",
    "cf_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac24d2b",
   "metadata": {},
   "source": [
    "We can see that our policy achieves a low CF metric value, which indicates it is close to being perfectly counterfactually fair. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d221c98",
   "metadata": {},
   "source": [
    "## Bonus: Assessing a Fairness-through-unawareness Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c20148",
   "metadata": {},
   "source": [
    "Fairness-through-unawareness proposes to ensure fairness by excluding the sensitive attribute from the state variable. Nevertheless, it has been argued that this method can still be unfair because the agent might learn the bias indirectly from the states and rewards, which are often biased. In this section, we train a policy following fairness-through-unawareness using the same training trajectory data and estimate its value and CF metric.\n",
    "\n",
    "We begin by training a fairness-through-unawareness policy. In the code below, `agent_unaware` only uses `states_train`, `actions_train`, and `rewards_train` during training, which enforces fairness-through-unwareness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "883047f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:50<00:00,  1.97it/s]\n"
     ]
    }
   ],
   "source": [
    "agent_unaware = FQI(num_actions=2, model_type='nn', preprocessor=None)\n",
    "agent_unaware.train(zs=zs_train, \n",
    "                    xs=states_train, \n",
    "                    actions=actions_train, \n",
    "                    rewards=rewards_train, \n",
    "                    max_iter=100, \n",
    "                    preprocess=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614bff4a",
   "metadata": {},
   "source": [
    "We now estimate the value of the fairness-through-unawareness policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9ef3b35f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [01:15<00:00,  2.66it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-58.894543"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_unaware = evaluate_reward_through_fqe(zs=zs_test, \n",
    "                                            states=states_test, \n",
    "                                            actions=actions_test, \n",
    "                                            rewards=rewards_test, \n",
    "                                            policy=agent_unaware, \n",
    "                                            model_type='nn')\n",
    "value_unaware"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5461ff98",
   "metadata": {},
   "source": [
    "Finally, we estimate the CF metric of the fairness-through-unawareness policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "62b43fcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5527272727272727"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cf_metric_unaware = evaluate_fairness_through_model(env=env, \n",
    "                                                    zs=zs_test, \n",
    "                                                    states=states_test, \n",
    "                                                    actions=actions_test, \n",
    "                                                    policy=agent_unaware)\n",
    "cf_metric_unaware"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c98dd02",
   "metadata": {},
   "source": [
    "We can see that the fairness-through-unawareness policy is much less fair than the policy learned using the preprocessed trajectory. This implies the preprocessing method effectively reduced the bias in the training trajectory. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
