{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d541a30b",
   "metadata": {},
   "source": [
    "# Assessing Preprocessors Using Synthetic Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa50e429",
   "metadata": {},
   "source": [
    "In this workflow, CFRL first uses `sample_trajectory()` to sample a trajectory from a \n",
    "`SyntheticEnvironment` whose transition rules are pre-specified. It then preprocesses the \n",
    "sampled trajectory using some custom preprocessor defined by the user. \n",
    "After that, the preprocessed trajectory is passed into `FQI` to train a policy, which is then \n",
    "assessed using synthetic data via `evaluate_reward_through_simulation()` and \n",
    "`evaluate_fairness_through_simulation()`. The final output of the workflow is the policy trained \n",
    "on the preprocessed data as well as its estimated value and counterfactual fairness metric. This \n",
    "workflow is appropriate when the user wants to examine the impact of some trajectory preprocessing \n",
    "method on the value and counterfactual fairness of the trained policy.\n",
    "\n",
    "We begin by importing the liberaries needed for this demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce542a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need this temporarily to import CFRL before it is officially published to PyPI\n",
    "import sys\n",
    "sys.path.append(\"E:/learning/university/MiSIL/CFRL Python Package/CFRL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65336cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import pytorch as torch\n",
    "from CFRL.preprocessor import Preprocessor\n",
    "from CFRL.agents import FQI\n",
    "from CFRL.environment import SyntheticEnvironment, sample_trajectory\n",
    "from CFRL.evaluation import evaluate_reward_through_simulation\n",
    "from CFRL.evaluation import evaluate_fairness_through_simulation\n",
    "from examples.baseline_agents import RandomAgent\n",
    "np.random.seed(1) # ensure reproducibility\n",
    "#torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd63df0",
   "metadata": {},
   "source": [
    "## Demonstration Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e7135f",
   "metadata": {},
   "source": [
    "Suppose we have a preprocessor `ConcatenatePreprocessor`, which is defined in the code block below. It essentially adds the senstive attribute to the state variable, which means the policy will directly use the senstive attribute during decision-making. We want to assess how this preprocessing method performs in terms of the value and counterfactual fairness of resulting policies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4355de68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConcatenatePreprocessor(Preprocessor):\n",
    "        def __init__(self) -> None:\n",
    "            pass\n",
    "\n",
    "        def preprocess(\n",
    "                self, \n",
    "                z: list | np.ndarray, \n",
    "                xt: list | np.ndarray\n",
    "            ) -> tuple[np.ndarray]:\n",
    "            if xt.ndim == 1:\n",
    "                xt = xt[np.newaxis, :]\n",
    "                z = z[np.newaxis, :]\n",
    "                xt_new = np.concatenate([xt, z], axis=1)\n",
    "                return xt_new.flatten()\n",
    "            elif xt.ndim == 2:\n",
    "                xt_new = np.concatenate([xt, z], axis=1)\n",
    "                return xt_new\n",
    "            \n",
    "        def preprocess_single_step(\n",
    "                self, \n",
    "                z: list | np.ndarray, \n",
    "                xt: list | np.ndarray, \n",
    "                xtm1: list | np.ndarray | None = None, \n",
    "                atm1: list | np.ndarray | None = None, \n",
    "                rtm1: list | np.ndarray | None = None, \n",
    "                verbose: bool = False\n",
    "            ) -> tuple[np.ndarray, np.ndarray] | np.ndarray:\n",
    "            z = np.array(z)\n",
    "            xt = np.array(xt)\n",
    "            if verbose:\n",
    "                print(\"Preprocessing a single step...\")\n",
    "\n",
    "            xt_new = self.preprocess(z, xt)\n",
    "            if rtm1 is None:\n",
    "                return xt_new\n",
    "            else:\n",
    "                return xt_new, rtm1\n",
    "            \n",
    "\n",
    "        def preprocess_multiple_steps(\n",
    "                self, \n",
    "                zs: list | np.ndarray, \n",
    "                xs: list | np.ndarray, \n",
    "                actions: list | np.ndarray, \n",
    "                rewards: list | np.ndarray | None = None, \n",
    "                verbose: bool = False\n",
    "            ) -> tuple[np.ndarray, np.ndarray] | np.ndarray:\n",
    "            zs = np.array(zs)\n",
    "            xs = np.array(xs)\n",
    "            actions = np.array(actions)\n",
    "            rewards = np.array(rewards)\n",
    "            if verbose:\n",
    "                print(\"Preprocessing multiple steps...\")\n",
    "        \n",
    "            # some convenience variables\n",
    "            N, T, xdim = xs.shape\n",
    "            \n",
    "            # define the returned arrays; the arrays will be filled later\n",
    "            xs_tilde = np.zeros([N, T, xdim + zs.shape[-1]])\n",
    "            rs_tilde = np.zeros([N, T - 1])\n",
    "\n",
    "            # preprocess the initial step\n",
    "            np.random.seed(0)\n",
    "            xs_tilde[:, 0, :] = self.preprocess_single_step(zs, xs[:, 0, :])\n",
    "\n",
    "            # preprocess subsequent steps\n",
    "            if rewards is not None:\n",
    "                for t in range (1, T):\n",
    "                    np.random.seed(t)\n",
    "                    xs_tilde[:, t, :], rs_tilde[:, t-1] = self.preprocess_single_step(zs, \n",
    "                                                                                    xs[:, t, :], \n",
    "                                                                                    xs[:, t-1, :], \n",
    "                                                                                    actions[:, t-1], \n",
    "                                                                                    rewards[:, t-1]\n",
    "                                                                                    )\n",
    "                return xs_tilde, rs_tilde                \n",
    "            else:\n",
    "                for t in range (1, T):\n",
    "                    np.random.seed(t)\n",
    "                    xs_tilde[:, t, :] = self.preprocess_single_step(zs, \n",
    "                                                                    xs[:, t, :], \n",
    "                                                                    xs[:, t-1, :], \n",
    "                                                                    actions[:, t-1]\n",
    "                                                                    )\n",
    "                return xs_tilde"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e98ac0c",
   "metadata": {},
   "source": [
    "Meanwhile, we also define the environment in which we want to assess `ConcatenatePreprocessor`. We define the transition rules of the environment as follows, which has bivariate sensitive attributes and tri-variate states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50561f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# an environment with bivariate zs and trivariate states\n",
    "# x0_i = 0.5 + zs_1 + zs_2 + ux0_i (assuming z_coef=1)\n",
    "def f_x0_multi(\n",
    "        zs: np.ndarray, \n",
    "        ux0: np.ndarray, \n",
    "        z_coef: int = 1\n",
    "    ) -> np.ndarray:\n",
    "    gamma0 = np.array([np.repeat(np.array([0.5]), repeats=3), \n",
    "                       np.repeat(np.array(1 * z_coef), repeats=3), \n",
    "                       np.repeat(np.array(1 * z_coef), repeats=3)])\n",
    "    n = zs.shape[0]\n",
    "    M = np.concatenate(\n",
    "        [\n",
    "            np.ones([n, 1]),\n",
    "            zs,\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "    x0 = M @ gamma0\n",
    "    x0 = x0.reshape(-1, 3)\n",
    "    x0 = x0 + ux0\n",
    "    return x0\n",
    "\n",
    "# xt_i = -0.5 + (zs_1 - 0.5) + (zs_2 - 0.5) + 0.3 * (xtm1_1 + xtm1_2 + xtm1_3) \n",
    "# + 0.2 * (atm1 - 0.5) + 0.3 * (zs_1 - 0.5) * (atm1 - 0.5) \n",
    "# + 0.3 * (zs_1 - 0.5) * (atm1 - 0.5) + uxt (assuming z_coef=1)\n",
    "def f_xt_multi(\n",
    "        zs: np.ndarray, \n",
    "        xtm1: np.ndarray, \n",
    "        atm1: np.ndarray, \n",
    "        uxt: np.ndarray, \n",
    "        z_coef: int = 1\n",
    "    ) -> np.ndarray:\n",
    "    gamma = np.array([np.repeat(np.array([-0.5]), repeats=3), \n",
    "                      np.repeat(np.array(1 * z_coef), repeats=3), \n",
    "                      np.repeat(np.array(1 * z_coef), repeats=3), \n",
    "                      np.array([0.3, 0.3, 0.3]),\n",
    "                      np.array([0.3, 0.3, 0.3]), \n",
    "                      np.array([0.3, 0.3, 0.3]), \n",
    "                      np.array([0.2, 0.2, 0.2]), \n",
    "                      np.array([0.3, 0.3, 0.3]), \n",
    "                      np.array([0.3, 0.3, 0.3])])\n",
    "    n = xtm1.shape[0]\n",
    "    M = np.concatenate(\n",
    "        [\n",
    "            np.ones([n, 1]),\n",
    "            zs - 0.5,\n",
    "            xtm1,\n",
    "            atm1.reshape(-1, 1) - 0.5, \n",
    "            (zs[:, 0].reshape(-1, 1) - 0.5) * (atm1.reshape(-1, 1) - 0.5), \n",
    "            (zs[:, 1].reshape(-1, 1) - 0.5) * (atm1.reshape(-1, 1) - 0.5)\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "    xt = M @ gamma\n",
    "    xt = xt.reshape(-1, 3)\n",
    "    xt = xt + uxt\n",
    "    return xt\n",
    "\n",
    "# rt_i = -0.5 + zs_1 + zs_2 + xt_1 + xt_2 + xt_3 + at + urt (assuming z_coef=1)\n",
    "def f_rt_multi(\n",
    "        zs: np.ndarray, \n",
    "        xt: np.ndarray, \n",
    "        at: np.ndarray, \n",
    "        urtm1: np.ndarray, \n",
    "        z_coef: int = 1\n",
    "    ) -> np.ndarray:\n",
    "    lmbda = np.array([-0.5, 1, 1, 1, 1 * z_coef, 1 * z_coef, 1, 1])\n",
    "    n = xt.shape[0]\n",
    "    at = at.reshape(-1, 1)\n",
    "    M = np.concatenate(\n",
    "        [np.ones([n, 1]), xt, zs, at, urtm1], axis=1\n",
    "    )\n",
    "    rt = M @ lmbda\n",
    "    return rt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d6865a",
   "metadata": {},
   "source": [
    "## Training Trajectory Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6153124",
   "metadata": {},
   "source": [
    "We now generate a trajectory with 300 inviduals (i.e. $N=300$) and 10 transitions used for training the `FQI` agent. Note that we do not train the preprocessor here because `ConcatenatePreprocessor` does not require training.\n",
    "\n",
    "We first initialize a `SyntheticEnvironment` using the transition rules defined in the previous section. This `SyntheticEnvironment` will be used to generate trajectories throughout this demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c0b2fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SyntheticEnvironment(state_dim=3, \n",
    "                           z_coef=1, \n",
    "                           f_x0=f_x0_multi, \n",
    "                           f_xt=f_xt_multi, \n",
    "                           f_rt=f_rt_multi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b1ebe8",
   "metadata": {},
   "source": [
    "Before generating the trajectory, we first generate the sensitive attributes of the 300 individuals in the trajectory. We allow the senstive attributes to take on four different values: $[0, 0]$, $[0, 1]$, $[1, 0]$, and $[1, 1]$. Each individual's sensitive attribute value is sampled randomly from a uniform distribution over all the legit sensitive attribute values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce44ced7",
   "metadata": {},
   "outputs": [],
   "source": [
    "zs_in = np.zeros((300, 2))\n",
    "z_levels = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "Z_idx = np.random.choice(range(z_levels.shape[0]), size=300, replace=True)\n",
    "for i in range(300):\n",
    "    zs_in[i] = z_levels[Z_idx[i]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e1a2c5",
   "metadata": {},
   "source": [
    "We now generate the trajectory using the `sample_trajectory()` function. The actions in the trajectory are taken using a random agent that selects 0 and 1 with equal probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "764a065f",
   "metadata": {},
   "outputs": [],
   "source": [
    "behavior_agent = RandomAgent(2)\n",
    "zs, states, actions, rewards = sample_trajectory(env=env, \n",
    "                                                 zs=zs_in, \n",
    "                                                 state_dim=3, \n",
    "                                                 T=10, \n",
    "                                                 policy=behavior_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a385e2",
   "metadata": {},
   "source": [
    "We check the shapes of Trajectory Arrays generated by `sample_trajectory()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ee7ef10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((300, 2), (300, 11, 3), (300, 10), (300, 10))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zs.shape, states.shape, actions.shape, rewards.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafdf2d5",
   "metadata": {},
   "source": [
    "## Policy Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bfaa60",
   "metadata": {},
   "source": [
    "We now learn a policy using `FQI` and `ConcatenatePreprocessor`. We first initialize an `FQI` agent that uses `ConcatenatePreprocessor` as its internal preprocessor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ed4f338",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = ConcatenatePreprocessor()\n",
    "agent = FQI(num_actions=2, model_type='nn', preprocessor=cp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f3af5f",
   "metadata": {},
   "source": [
    "We now perform training. Since we set `preprocess=True` in `train()`, `agent` will use its internal preprocessor (i.e. `cp`) to automatically preprocess the input training trajectory before using the trajectory for policy learning. Therefore, we can directly pass in the unpreprocessed states and rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bef15eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:46<00:00,  2.17it/s]\n"
     ]
    }
   ],
   "source": [
    "agent.train(zs=zs, \n",
    "            xs=states, \n",
    "            actions=actions, \n",
    "            rewards=rewards, \n",
    "            max_iter=100, \n",
    "            preprocess=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "633fad9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.act(z=zs, xt=states[:, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2722f2a",
   "metadata": {},
   "source": [
    "## Value Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a951ec0f",
   "metadata": {},
   "source": [
    "We now estimate the value achieved by the trained policy when interacting with the target environment (i.e. `env`). Since the underlying transition rules are known, we can directly use `evaluate_rewards_through_simulation()`. This function generates a new trajectory using `agent` under `env` and computes the discounted cumulative rewards collected in the trajectory.\n",
    "\n",
    "We evaluate the discounted cumulative rewards using a simulation with 100 individuals (i.e. $N=100$) and 500 transitions ($T=500$). The discount factor is $\\gamma=0.9$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1fec81e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-16.296371287239186"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value = evaluate_reward_through_simulation(env=env, \n",
    "                                           z_eval_levels=[[0, 0], [0, 1], \n",
    "                                                          [1, 0], [1, 1]], \n",
    "                                           state_dim=3, \n",
    "                                           N=100, \n",
    "                                           T=500, \n",
    "                                           policy=agent)\n",
    "value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1920078",
   "metadata": {},
   "source": [
    "## Counterfactual Fairness Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144fc023",
   "metadata": {},
   "source": [
    "We now estimate the counterfactual fairness acheived by the policy when interacting with the target environment (i.e. `env`). To do so, we use `evaluate_fairness_through_simulation()`. This function first generates the counterfactual trajectories of each individual in the data under a set legit sensitive attribute values using the policy that is to be evaluated. It then  calculates and returns a counterfactual fairness metric (CF metric) following the formula \n",
    "\n",
    "$\\max_{z', z \\in eval(Z)} \\frac{1}{NT} \\sum_{i=1}^{N} \\sum_{t=1}^{T} \\mathbb{I} \\left( A_t^{Z \\leftarrow z'}\\left(\\bar{U}_t(h_{it})\\right) \\neq A_t^{Z \\leftarrow z}\\left(\\bar{U}_t(h_{it})\\right) \\right),$\n",
    "\n",
    "where $eval(Z)$ is the set of sensitive attribute values passed in by `z_eval_levels`, $A_t^{Z \\leftarrow z'}\\left(\\bar{U}_t(h_{it})\\right)$ is the action taken in the counterfactual trajectory under $`Z=z'$, and $A_t^{Z \\leftarrow z}\\left(\\bar{U}_t(h_{it})\\right)$ is the action taken under the counterfactual trajectory under $Z=z$. The CF metric is bounded between 0 and 1, with 0 representing perfect fairness and 1 indicating complete unfairness.\n",
    "\n",
    "We evaluate the counterfactual fairness using a simulation with 100 individuals (i.e. $N=100$) and 10 transitions ($T=10$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e3282ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47000000000000003"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cf_metric = evaluate_fairness_through_simulation(env=env, \n",
    "                                                 z_eval_levels=[[0, 0], [0, 1], \n",
    "                                                                [1, 0], [1, 1]], \n",
    "                                                 state_dim=3, \n",
    "                                                 N=100, \n",
    "                                                 T=10, \n",
    "                                                 policy=agent)\n",
    "cf_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64eccddd",
   "metadata": {},
   "source": [
    "## SCRATCH WORK (SHOULD DELETE LATER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3a84577",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:22<00:00, 44.34it/s]\n",
      "100%|██████████| 1000/1000 [00:22<00:00, 43.89it/s]\n",
      "100%|██████████| 1000/1000 [00:21<00:00, 45.70it/s]\n",
      "100%|██████████| 1000/1000 [00:22<00:00, 44.74it/s]\n",
      "100%|██████████| 1000/1000 [00:21<00:00, 45.63it/s]\n"
     ]
    }
   ],
   "source": [
    "from CFRL.preprocessor import SequentialPreprocessor\n",
    "sp = SequentialPreprocessor(z_space=[[0, 0], [0, 1], [1, 0], [1, 1]], \n",
    "                            num_actions=2, \n",
    "                            cross_folds=5, \n",
    "                            mode='single', \n",
    "                            reg_model='nn')\n",
    "states_tilde, rewards_tilde = sp.train_preprocessor(zs=zs, \n",
    "                                                    xs=states, \n",
    "                                                    actions=actions, \n",
    "                                                    rewards=rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00a2e76f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:45<00:00,  2.22it/s]\n"
     ]
    }
   ],
   "source": [
    "agent_cf = FQI(num_actions=2, model_type='nn', preprocessor=sp)\n",
    "agent_cf.train(zs=zs, \n",
    "            xs=states_tilde, \n",
    "            actions=actions, \n",
    "            rewards=rewards_tilde, \n",
    "            max_iter=100, \n",
    "            preprocess=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e4905049",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_cf.act(z=zs, xt=states[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e8ac298e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-17.855928340263535"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value = evaluate_reward_through_simulation(env=env, \n",
    "                                           z_eval_levels=[[0, 0], [0, 1], \n",
    "                                                          [1, 0], [1, 1]], \n",
    "                                           state_dim=3, \n",
    "                                           N=100, \n",
    "                                           T=500, \n",
    "                                           policy=agent_cf)\n",
    "value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "026b1fd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.018000000000000002"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cf_metric = evaluate_fairness_through_simulation(env=env, \n",
    "                                                 z_eval_levels=[[0, 0], [0, 1], \n",
    "                                                                [1, 0], [1, 1]], \n",
    "                                                 state_dim=3, \n",
    "                                                 N=100, \n",
    "                                                 T=10, \n",
    "                                                 policy=agent_cf)\n",
    "cf_metric"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
